{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ff08aa-9221-4bba-b48a-73a05135429a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Polygon\n",
    "import pdb\n",
    "import  cv2\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd8c910b-ffae-46a0-a673-8b896ba0beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENSLIDE_PATH=Path('openslide-win64-20230414')\n",
    "\n",
    "OPENSLIDE_FOLDER=os.path.join(Path().resolve(),'openslide-win64-20230414','openslide-win64-20230414','bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5441f1db-0fa9-4e8b-9c38-4bebdd08d39d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if hasattr(os,'add_dll_directory'):\n",
    "    with os.add_dll_directory(OPENSLIDE_FOLDER):\n",
    "        import openslide\n",
    "else:\n",
    "    import openslide\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20326129-5a8a-4358-abff-4517b1277a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'openslide' from 'A:\\\\mini\\\\envs\\\\aipath\\\\Lib\\\\site-packages\\\\openslide\\\\__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db231725-36f9-4b6b-a114-8ee45c588184",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from dataloading.src.annotation_parser import AnnotationParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e08874d6-0df7-47bc-bebc-ec278eeee67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloading.src.annotation_parser import AnnotationParser\n",
    "from dataloading.src.wsi_datasets_tst import WSI_Pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3be9e30c-0201-49ae-981a-d05ef4664c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile src/annotation_parser.py\n",
    "from __future__ import annotations\n",
    "from shapely.geometry import Polygon\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import json\n",
    "import pdb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class AnnotationParser():\n",
    "    \"\"\" Parses WSI Files and associated GeoJson annotations to create a composite data frame\n",
    "         from both.Also returns the annotations as line items for which th eparsing didn't work \n",
    "         Requires folder containing WSI files and Annotations\"\"\"\n",
    "    \n",
    "    def __init__(self,image_path:pathlib.Path,labels_path:pathlib.Path)->None:\n",
    "        self.image_path=image_path\n",
    "        self.labels_path=labels_path\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_img_df(self)->pd.DataFrame:\n",
    "        \"\"\"use openslide to get properties,shape,levels etc.for each image\"\"\"\n",
    "        img_paths=[self.image_path/fn for fn in os.listdir(self.image_path)  if (self.image_path/fn).suffix == '.tiff']\n",
    "        img_df=pd.DataFrame({'image_path':img_paths})\n",
    "        img_df=img_df.assign(image_name=[img_path.stem for img_path in img_paths],\n",
    "                             WSI_size=[openslide.OpenSlide(img_path).dimensions for img_path in img_paths],\n",
    "                      levels=[openslide.OpenSlide(img_path).level_count for img_path in img_paths],\n",
    "                    downsample_levels=[{level:downsample for level,downsample in enumerate(openslide.OpenSlide(img_path).level_downsamples)} \n",
    "                           for img_path in img_paths])\n",
    "        return img_df\n",
    "\n",
    "\n",
    "    \n",
    "    def get_coordinates_array(self,anno_row:pd.Series)->np.ndarray:\n",
    "        \"\"\"parse the annotation json (as a series of rows to get coordinates\n",
    "           of the annotation as an array of dim n_points X 2 \"\"\"\n",
    "        \n",
    "       \n",
    "        \n",
    "        try:\n",
    "            geom=anno_row['geometry']\n",
    "            coord_list=geom['coordinates']\n",
    "            geom_type=geom['type']\n",
    "\n",
    "             ### last 2 dimensions of every poly are n_points X 2\n",
    "            # for multipolygon the list of coordinates i nested 1 level deep\n",
    "\n",
    "             ## to get largest polygon if more than one get marked by mistake in  every annotation\n",
    "            \n",
    "            if geom_type=='Polygon':\n",
    "                largest_poly=max([np.array(poly).reshape((np.array(poly).shape)[-2:]) for poly in coord_list],key=len)\n",
    "\n",
    "            if geom_type=='MultiPolygon':\n",
    "                largest_poly= max([max([np.array(poly).reshape((np.array(poly).shape)[-2:]) for poly in coords],key=len) \n",
    "                                   for coords in coord_list],key=len)\n",
    "\n",
    "            return largest_poly\n",
    "\n",
    "        \n",
    "        except KeyError:\n",
    "            return 'coordinates_key_error'\n",
    "        \n",
    "       \n",
    "    \n",
    "    def get_class_name_and_color(self,anno_row:pd.Series)->Tuple[str,int]:\n",
    "        \"\"\"Parse annotation to return class name for each anno and color assigned in QuPath\"\"\"\n",
    "        \n",
    "     \n",
    "        \n",
    "        try:\n",
    "            classification=anno_row['properties']['classification']\n",
    "            class_name=classification['name']\n",
    "            class_color=classification['color']\n",
    "            return class_name,np.array(class_color)\n",
    "        \n",
    "        except KeyError:\n",
    "            \n",
    "            \n",
    "            return 'class_name_error',0\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "    def parse_json_file(self,json_path:pathlib.Path):\n",
    "        # need to be list of dicts (anno_list)\n",
    "        with open(json_path) as json_file:\n",
    "            anno_list = json.load(json_file)\n",
    "            if not isinstance(anno_list, list):\n",
    "                anno_list=[anno_list]\n",
    "            \n",
    "        \n",
    "        anno_df=pd.DataFrame(anno_list)\n",
    "        anno_df=anno_df.assign(image_name=json_path.stem)\n",
    "        #pdb.set_trace()\n",
    "        return anno_df\n",
    "\n",
    "        \n",
    "    \n",
    "    def get_anno_df(self,img_df:pd.DataFrame)->Tuple[pd.DataFrame,pd.DataFrame]:\n",
    "        \n",
    "        \"\"\" get annotation df including annotation for which parsing did not work (errored)\"\"\"\n",
    "        \n",
    "        jsons=[self.labels_path/fn for fn in os.listdir(self.labels_path)  if (self.labels_path/fn).suffix == '.geojson']\n",
    "        anno_df=pd.concat([self.parse_json_file(json_path) for json_path in jsons])\n",
    "        #pdb.set_trace()\n",
    "        #anno_df=pd.concat([pd.read_json(json,orient='records').assign(image_name=json.stem) for json in jsons],ignore_index=True)\n",
    "        \n",
    "        ## enrich with image specific attrs, total size,zoom levels etc.\n",
    "        anno_df=anno_df.merge(img_df,on='image_name')\n",
    "        ## add coordinates as np array and shapely polygon with transormed origin \n",
    "        anno_df['coordinates']=anno_df.apply(self.get_coordinates_array,1)\n",
    "        #pdb.set_trace()\n",
    "        anno_df['class_name'], anno_df['colour_RGB']=zip(*anno_df.apply(self.get_class_name_and_color,1))\n",
    "        \n",
    "        ## select the annos with errored coordinates (due to annotation issues)\n",
    "        errored=np.logical_or(anno_df['coordinates'].isin(['coordinates_key_error']),\n",
    "                              anno_df['class_name']=='class_name_error')\n",
    "        \n",
    "        errored_df=anno_df[errored]\n",
    "        anno_df=anno_df[~errored]\n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        ## use shapely to compute polygon attrs\n",
    "        anno_df['polygon']=anno_df.apply(lambda x:Polygon(x['coordinates']),1)\n",
    "        anno_df['area']=anno_df.apply(lambda x:x['polygon'].area,1)\n",
    "        anno_df['circumference']=anno_df.apply(lambda x:x['polygon'].length,1)\n",
    "        anno_df['bounds']=anno_df.apply(lambda x:np.array(x['polygon'].bounds).reshape((2,2)),1)\n",
    "       \n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        return anno_df,errored_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    def parse_annotations(self)->Tuple[pd.DataFrame,pd.DataFrame,pd.DataFrame]:\n",
    "        \"\"\" returns anno_df,img_df and errored df in that order \"\"\" \n",
    "        img_df=self.get_img_df()\n",
    "        anno_df,errored_df=self.get_anno_df(img_df)\n",
    "        \n",
    "        \n",
    "        return anno_df,img_df,errored_df\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d5f731d-4075-40e4-9c1f-f8db12b4fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile src/wsi_datasets.py\n",
    "from __future__ import annotations\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from shapely.geometry import Polygon,MultiPolygon\n",
    "import  cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class WSI_Pyramid(Dataset):\n",
    "\n",
    "\n",
    "    \"\"\"Pytorch Dataset class representing a multiscale WSI dataset.inputs are img and anno dfs containing info \n",
    "         about WSI images and annotations.Pyramidal crops are sampled from the pyramid_top_levels (usually set to the most\n",
    "         zoomed in level in the tiff dataset although more than one level can be used) ,with a downsample factor of one . A crop is \n",
    "         chosen from the top level of size crop_sz X crop_sz and concentric crops of the same size are chosen in the next\n",
    "         num_pyramid_level levels.\"\"\"\n",
    "   \n",
    "    \n",
    "    def __init__(self,\n",
    "                 anno_df:pd.DataFrame,\n",
    "                 crop_pixel_size:tuple=(512,512),\n",
    "                 transform=None,\n",
    "                 class2num={'Background':0,'Tumor':1},\n",
    "                 \n",
    "                 ## default set to show all levels, mostly {0:1} top level is picked\n",
    "                 pyramid_top_level={0:1.0},\n",
    "                 num_pyramid_levels=4, \n",
    "                 num_pyramid_mask_levels=1,\n",
    "                 filter_flag=False)->None:\n",
    "       \n",
    "        \n",
    "       \n",
    "        self.anno_df=anno_df\n",
    "        ## the size in pixel of  each crop -size is kept same at \n",
    "        ## various zoom levels for batching\n",
    "        self.crop_pixel_size=crop_pixel_size\n",
    "        self.item_transform = transform\n",
    "        self.class2num= class2num\n",
    "        ## create on self.device\n",
    "        ## offsets to add to crop center to get vertices\n",
    "        self.offsets=torch.tensor(self.crop_pixel_size)//2\n",
    "        ## get the downsample levels common in the entire dataset\n",
    "        self.common_downsample_levels=min(self.anno_df['downsample_levels'],key=len)\n",
    "        ## take the intersect of user provided ds levels and the ones present in the data\n",
    "        self.pyramid_top_level=pyramid_top_level\n",
    "        self.pyramid_all_levels=max(self.anno_df['downsample_levels'],key=len)\n",
    "\n",
    "        \n",
    "        self.pyramid_top_idx=list(self.pyramid_top_level.keys())[0]\n",
    "        self.pyramid_top_downsample=list(self.pyramid_top_level.values())[0]\n",
    "        \n",
    "        self.num_pyramid_levels=num_pyramid_levels\n",
    "        self.num_pyramid_mask_levels=num_pyramid_mask_levels\n",
    "        assert self.num_pyramid_levels>= self.num_pyramid_mask_levels,'num_pyramid_levels used for inputs should be more than num of target maska'\n",
    "        \n",
    "        ## the actual levels of the tiff pyramid used as input to the model\n",
    "        self.pyramid_zoom_levels={idx:self.common_downsample_levels[idx] for idx in range(self.pyramid_top_idx,self.pyramid_top_idx+self.num_pyramid_levels)}\n",
    "        \n",
    "        self.filter_flag=filter_flag\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.anno_df)\n",
    "    \n",
    "    \n",
    "    def get_pyramid_crops(self,annotation_row:pd.Series):\n",
    "        \n",
    "        \"\"\" get a random center crop at any possible zoom level from the periphery\n",
    "           of an annotation \"\"\"\n",
    "       \n",
    "\n",
    "       \n",
    "       \n",
    "        wsi_size,anno_coordinates=(np.array(x) for x in [annotation_row['WSI_size'],\n",
    "                                                             annotation_row['coordinates']])\n",
    "\n",
    "        random_crop_index=np.random.randint(0,len(anno_coordinates))\n",
    "        random_crop_center=torch.tensor(anno_coordinates[random_crop_index])\n",
    "        # pdb.set_trace()\n",
    "        offsets_arr=torch.tensor([[-1,1],[1,1],[1,-1],[-1,-1]])*self.offsets   # 4 X 2\n",
    "        downsample_arr=torch.tensor(list(self.pyramid_zoom_levels.values())).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "       \n",
    "        pyramid_crops=offsets_arr.unsqueeze(0)*downsample_arr+random_crop_center.unsqueeze(0).unsqueeze(0)# Pyramid_Levels X 4 X 2  (one crop for each level)\n",
    "        pyramid_crops=np.array(pyramid_crops).astype(np.int32)\n",
    "\n",
    "        pyramid_top_lefts=pyramid_crops.min(axis=1)   # Pyramid_Levels X 2\n",
    "      \n",
    "       \n",
    "        ## get the top left of every sampled level in the pyramidal tiff by subtracting offset from crop center and scaling by downsample factor\n",
    "        #sampled_top_lefts={idx:random_crop_center-self.offsets*d_factor for idx,d_factor in self.pyramid_zoom_levels.items()}\n",
    "\n",
    "        return pyramid_crops,pyramid_top_lefts\n",
    "\n",
    "\n",
    "    def filter_crops_byWSIsize(self,wsi_size:tuple,all_crops:np.ndarray):\n",
    "        \n",
    "         \n",
    "    \n",
    "        max_bounds=np.max(all_crops,axis=1).values<wsi_size.unsqueeze(0)\n",
    "        min_bounds=np.min(all_crops,axis=1).values>torch.zeros_like(wsi_size.unsqueeze(0))\n",
    "        \n",
    "        ## get all feasible crops/tiles which are wholly within the WSI bounds, associated with that particular annotation\n",
    "        all_crops=all_crops[np.logical_and(max_bounds.all(axis=1),min_bounds.all(axis=1))]\n",
    "\n",
    "        ## if there is no possible crop that fits in the WSI image for a particular annotation and zoom level, retrn \n",
    "        ## empty tensors\n",
    "        \n",
    "         \n",
    "        return all_crops\n",
    "    \n",
    "   \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "    def get_mask_per_class(self,class_annotation_data:pd.DataFrame,crop:np.ndarray,\n",
    "                          downsample_factor:float)->torch.tensor:\n",
    "       \n",
    "        \"\"\"\"function to create masks of each class given the annotation data and crop(image) \n",
    "            coordinates=(4X2 shape) also the donsample factor of the crop to scale the polygon coords\"\"\"\n",
    "        annotation_class=class_annotation_data['class_name'].iloc[0]\n",
    "        annotation_num=self.class2num[annotation_class]\n",
    "        \n",
    "        \n",
    "        ## select the top left point of the crop\n",
    "        ## its the point with the min X and Y corrdinates (top left of image is origin)\n",
    "        \n",
    "        top_left=crop.min(axis=0)\n",
    "        \n",
    "        ## create a shapely polygon from crop to find intersections between annotations and crop\n",
    "        \n",
    "        crop_poly=Polygon(crop)\n",
    "        \n",
    "        ## create list of intersecting polygons with crop to fill with clss encoding\n",
    "        \n",
    "        intersects=[]\n",
    "        for poly in  class_annotation_data['polygon']:\n",
    "            if not crop_poly.intersects(poly):\n",
    "                continue\n",
    "            else:\n",
    "                intersect=crop_poly.intersection(poly)\n",
    "                \n",
    "                if isinstance(intersect,MultiPolygon):\n",
    "                    for inter in intersect.geoms:\n",
    "                        ext_coords=((np.array(inter.convex_hull.exterior.coords)-top_left)//downsample_factor).astype(np.int32)\n",
    "                        intersects.append(ext_coords)\n",
    "                elif isinstance(intersect,Polygon):\n",
    "                        ext_coords=((np.array(intersect.convex_hull.exterior.coords)-top_left)//downsample_factor).astype(np.int32)\n",
    "                        intersects.append(ext_coords)\n",
    "                else:\n",
    "                        continue\n",
    "                        \n",
    "                        \n",
    "                \n",
    "\n",
    "                    \n",
    "        mask=np.zeros(self.crop_pixel_size,dtype=np.uint8)\n",
    "        \n",
    "       \n",
    "        ## fill the intersected polygons within the mask\n",
    "        cv2.fillPoly(mask,intersects,color=annotation_num)\n",
    "        \n",
    "        return torch.tensor(mask,dtype=torch.uint8)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def read_slide_region(self,slide_obj:openslide.OpenSlide,top_left:np.ndarray,\n",
    "                         level:int):\n",
    "        \"\"\" returns the pixel RGB from WSI given a location,crop_size and level\"\"\"\n",
    "        #pdb.set_trace()\n",
    "        return slide_obj.read_region(tuple(top_left.astype(np.int32)),level,self.crop_pixel_size)\n",
    "\n",
    "    def get_dl(self,batch_size,kind,shuffle=True):\n",
    "        ## only shuffle the train dl not the validation one\n",
    "        shuffle=kind=='train'\n",
    "        return DataLoader(dataset=self,batch_size=batch_size,shuffle=shuffle)\n",
    "\n",
    "    def get_img_T(self,pyramid_top_lefts:np.ndarray,image_path:pathlib.Path):\n",
    "\n",
    "        img_T=np.concatenate([np.array(self.read_slide_region(openslide.OpenSlide(image_path),sampled_top_left,zoom_level))[:,:,:-1] \n",
    "                  for zoom_level,sampled_top_left in zip(self.pyramid_zoom_levels,pyramid_top_lefts)],axis=2)\n",
    "        img_T=torch.tensor(img_T).permute(2,0,1)\n",
    "        return img_T\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def get_msk_T(self,pyramid_crops:np.ndarray,image_anno_data:pd.DataFrame):\n",
    "        pyramid_msk=[]\n",
    "        for i in range(self.num_pyramid_mask_levels):\n",
    "            get_classwise_masks=partial(self.get_mask_per_class,\n",
    "                                              crop=pyramid_crops[i],\n",
    "                                              downsample_factor= list(self.pyramid_zoom_levels.values())[i])\n",
    "            \n",
    "            class_wise_masks=image_anno_data.groupby('class_name').apply(get_classwise_masks)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            ## stack the masks of various classes\n",
    "            stacked_masks=torch.stack(class_wise_masks.to_list(),dim=0)\n",
    "            \n",
    "            ## create a composite mask with higher class numbers taking precedence in case of ties\n",
    "            \n",
    "            composite_mask=stacked_masks.max(dim=0)\n",
    "            pyramid_msk.append(composite_mask.values)\n",
    "        \n",
    "        return torch.stack(pyramid_msk)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ## select annotation \n",
    "        anno_row=self.anno_df.iloc[index]\n",
    "        \n",
    "        ## select all annotations in the same image as indexed annotation\n",
    "        image_name,anno_class=anno_row['image_name'],anno_row['class_name']\n",
    "        dowsample_levels=anno_row['downsample_levels']\n",
    "        image_path=anno_row['image_path']\n",
    "        image_anno_data=self.anno_df[self.anno_df['image_name']==image_name]\n",
    "        \n",
    "        ## select pyramidal crops from N_levels zoom levels\n",
    "        pyramid_crops,pyramid_top_lefts=self.get_pyramid_crops(anno_row)\n",
    "\n",
    "        ## create a stack of pyramid crops centered at the annotation with as many zoom levels as descibed by pyramid_levels\n",
    "        img_T=self.get_img_T(pyramid_top_lefts,image_path)\n",
    "        mask_T=self.get_msk_T(pyramid_crops,image_anno_data)\n",
    "    \n",
    "        return img_T,mask_T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class WSI_Inference(WSI_Pyramid):\n",
    "\n",
    "\n",
    "    \"\"\"Pytorch Dataset class to perform inference on a WSI.Input is the tissue locations on a WSI which are obtained after\n",
    "      removal of background. Inference is run on crops of 128X128 extracted from these locations.Inherits from the pyramid\n",
    "      parent class to make available common convenience functions\"\"\"\n",
    "\n",
    "    \n",
    "   \n",
    "    \n",
    "    def __init__(self,\n",
    "                 wsi_path:pathlib.Path,\n",
    "                 wsi_tissue_locs:np.ndarray,\n",
    "                 **kwargs)->None:\n",
    "        ## init the pyramidal dataset\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.wsi_path= wsi_path\n",
    "        self.wsi_tissue_locs=wsi_tissue_locs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len( self.wsi_tissue_locs)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        top_left=torch.tensor(self.wsi_tissue_locs[index])\n",
    "        crop_center=top_left+self.offsets\n",
    "        downsample_arr=torch.tensor(list(self.pyramid_zoom_levels.values()))\n",
    "        pyramid_top_lefts=crop_center-self.offsets.unsqueeze(0)* downsample_arr.unsqueeze(1)\n",
    "        pyramid_top_lefts=np.array( pyramid_top_lefts).astype(np.int32)\n",
    "        img_T=self.get_img_T(pyramid_top_lefts,self.wsi_path)\n",
    "        ## return locations and pyramidal images for inference\n",
    "        return top_left,img_T\n",
    "\n",
    "    def get_dl(self,batch_size):\n",
    "        return DataLoader(dataset=self,batch_size=batch_size,shuffle=False)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "       \n",
    "        \n",
    "       \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e0d2e69-a276-4a1a-be1f-89b90c6735f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\mini\\envs\\aipath\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "A:\\mini\\envs\\aipath\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#%%writefile src/segmodule.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import Accuracy,Precision, Recall,JaccardIndex,Dice\n",
    "\n",
    "\n",
    "\n",
    "# Define your PyTorch Lightning module (inherits from pl.LightningModule)\n",
    "class SegLightningModule(pl.LightningModule):\n",
    "    def __init__(self,in_channels=12,num_classes=2, arch_name='UnetPlusPlus'\n",
    "                 ,encoder_name='resnet34',crossentropy_weights=(3.0,8.0),lr=1e-3,**kwargs):\n",
    "        super(SegLightningModule, self).__init__()\n",
    "        # Define your model architecture here\n",
    "        \n",
    "        self.train_metrics = MetricCollection(prefix='Train',metrics=[\n",
    "                            Accuracy(task='binary' ),\n",
    "                            Precision(task='binary'),\n",
    "                            Recall(task='binary'),\n",
    "                           JaccardIndex(task='binary')])\n",
    "        self.val_metrics = MetricCollection(prefix='Val',metrics=[\n",
    "                            Accuracy(task='binary' ),\n",
    "                            Precision(task='binary'),\n",
    "                            Recall(task='binary'),\n",
    "                           JaccardIndex(task='binary')])\n",
    "        \n",
    "        arch=getattr(smp, arch_name)\n",
    "        self.segmentation_model=arch(encoder_name=encoder_name,in_channels=in_channels,classes=num_classes)\n",
    "        self.loss_fn=nn.CrossEntropyLoss(weight=torch.tensor(crossentropy_weights))\n",
    "        self.lr=lr\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass of your model\n",
    "        \n",
    "        return self.segmentation_model(x)\n",
    "    \n",
    "    def flattened_cross_entropy_loss(self,inp,tgt):\n",
    "        \n",
    "        tgt=tgt.flatten(start_dim=-2)\n",
    "        inp=inp.flatten(start_dim=-2)\n",
    "      \n",
    "        return self.loss_fn(inp, tgt)\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Define the training step\n",
    "      \n",
    "        img_b,mask_b=batch\n",
    "        img_b=img_b.to(torch.float32)\n",
    "        #taking the mask at the highest zoom  as target\n",
    "        mask_b=mask_b[:,0,:,:].to(torch.int64) \n",
    "        \n",
    "        y_pred_logits = self(img_b)\n",
    "        loss = self.flattened_cross_entropy_loss(y_pred_logits, mask_b)\n",
    "        y_pred=y_pred_logits.argmax(axis=1)\n",
    "        \n",
    "        metrics=self.train_metrics(y_pred,mask_b)\n",
    "        metrics.update({'Train_loss':loss,'Train_pct_foreground':mask_b.float().mean()})\n",
    "        \n",
    "        \n",
    "        self.log_dict(metrics,on_step=False,on_epoch=True,prog_bar=False)  # Log the training loss for TensorBoard\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Define the training step\n",
    "      \n",
    "        img_b,mask_b=batch\n",
    "        img_b=img_b.to(torch.float32)\n",
    "        #taking the mask at the highest zoom  as target\n",
    "        mask_b=mask_b[:,0,:,:].to(torch.int64) \n",
    "        \n",
    "        y_pred_logits = self(img_b)\n",
    "        loss = self.flattened_cross_entropy_loss(y_pred_logits, mask_b)\n",
    "        y_pred=y_pred_logits.argmax(axis=1)\n",
    "        \n",
    "        metrics=self.val_metrics(y_pred,mask_b)\n",
    "        metrics.update({'Val_loss':loss,'Val_pct_foreground':mask_b.float().mean()})\n",
    "        self.log_dict(metrics,on_step=False,on_epoch=True,prog_bar=False)  # Log the training loss for TensorBoard\n",
    "        return loss\n",
    "        \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define your optimizer\n",
    "        optimizer = optim.Adam(self.parameters(),lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "class InferenceLightningModule(pl.LightningModule):\n",
    "    def __init__(self,seg_module:SegLightningModule,inference_fpath:pathlib.Path,**kwargs):\n",
    "       super(SegLightningModule, self).__init__()\n",
    "        # Define your model architecture here\n",
    "        \n",
    "       self.seg_module=seg_module\n",
    "       self.inference_fpath=inference_fpath\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass of your model\n",
    "        \n",
    "        return  self.seg_module(x)\n",
    "    \n",
    "    \n",
    "\n",
    "    def get_inference_schema(self,sample_json='inference_schema.geojson'):\n",
    "        json_path=self.inference_path/sample_json\n",
    "        with open(json_path) as json_file:\n",
    "             schema = json.load(json_file)\n",
    "\n",
    "        return schema\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Define the training step\n",
    "      \n",
    "        img_b,top_left_b=batch\n",
    "        img_b=img_b.to(torch.float32)\n",
    "       \n",
    "        y_pred_logits = self(img_b)\n",
    "        \n",
    "        y_pred=y_pred_logits.argmax(axis=1)\n",
    "        pred_coords=torch.cat([torch.argwhere(pred)+top_left for pred,top_left in zip(y_pred,top_left_b) if pred.sum()>0])\n",
    "     \n",
    "        \n",
    "       \n",
    "      \n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "796af1a7-3550-43e5-9a43-7019a0b52daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile src/runner.py\n",
    "from __future__ import annotations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar,EarlyStopping\n",
    "\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class experiment_runner():\n",
    "    \n",
    "    \n",
    "    def __init__(self,root=Path('training_data')\n",
    "                 \n",
    "                 \n",
    "               ):\n",
    "            \n",
    "            self.label_path=root/'labels'\n",
    "            self.image_path=root/'images'\n",
    "            \n",
    "            \n",
    "            self.inference_path=Path('inference')\n",
    "\n",
    "            self.inference_image_path=self.inference_path/'images'\n",
    "            ## path to store predictions (as json files)\n",
    "            self.inference_label_path=self.inference_path/'labels'\n",
    "            \n",
    "            self.parser=AnnotationParser(self.image_path,self.label_path)\n",
    "            self.anno_df,self.img_df,self.errored=self.parser.parse_annotations()\n",
    "            ## precompute tissue locations for training images\n",
    "            \n",
    "            t_locs=self.img_df['image_path'].apply(self.preprocess,1)\n",
    "            self.img_df=self.img_df.assign(tissue_location=t_locs)\n",
    "            self.img_df['inference_len']=self.img_df['tissue_location'].apply(len,1)\n",
    "            ## device to perform inference on\n",
    "            self.device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "            \n",
    "            \n",
    "          \n",
    "            \n",
    "    def preprocess(self,img_path:pathlib.Path,downsample_factor=512,gray_background=np.array([236, 236, 236]),tol=0)->np.ndarray:\n",
    "        \"\"\" preprocess WSI to remove grey areas returns pixel locations\n",
    "            from the downsampled (thumbnail) image where the tissue exists\n",
    "              \"\"\"\n",
    "        \n",
    "        \n",
    "        slide=openslide.OpenSlide(img_path)\n",
    "        H,W=slide.dimensions\n",
    "       \n",
    "        thumbnail_img=np.array(slide.get_thumbnail((H//downsample_factor,W//downsample_factor)))\n",
    "        h,w,c=thumbnail_img.shape\n",
    "        \n",
    "        rgb_upper_bound=gray_background+tol\n",
    "        rgb_lower_bound=gray_background-tol\n",
    "        grey_mask = cv2.inRange( thumbnail_img, rgb_lower_bound,  rgb_upper_bound)\n",
    "        \n",
    "        \n",
    "        num_comps, labelled_mask = cv2.connectedComponents(~grey_mask)\n",
    "\n",
    "        tissue_comps=[]\n",
    "        tissue_mask=[]\n",
    "        \n",
    "        for i in range(1,num_comps):\n",
    "            comp_mask=(labelled_mask==i)\n",
    "            #pdb.set_trace()\n",
    "            unique_rgb_vals=np.unique(thumbnail_img.reshape((h*w,c))[comp_mask.flatten()],axis=0)\n",
    "            if len(unique_rgb_vals)>1:\n",
    "               tissue_comps.append(np.argwhere(comp_mask))\n",
    "               tissue_mask.append(comp_mask)\n",
    "\n",
    "     \n",
    "                    \n",
    "                    \n",
    "            \n",
    "        return np.concatenate(tissue_comps)*downsample_factor\n",
    "        \n",
    "        \n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_dls(self,\n",
    "               batch_size=32,\n",
    "                num_pyramid_levels=4,\n",
    "                num_pyramid_mask_levels=1,\n",
    "                crop_pixel_size=(128,128),\n",
    "                pyramid_top_level={0: 1.0}\n",
    "                \n",
    "                ):\n",
    "\n",
    "            #filtering out image_name with less than 2 annotations for stratifying using train_test_split\n",
    "            vc=self.anno_df['image_name'].value_counts()\n",
    "            filt_df=self.anno_df.merge(vc.reset_index())\n",
    "            ## select images with more than the median number of annotations\n",
    "            filt_df=filt_df[filt_df['count']>50]\n",
    "            #filt_df=filt_df[filt_df['count']==max(vc)]\n",
    "           \n",
    "            df_train,df_val=train_test_split(filt_df,test_size=0.2,random_state=42,stratify=filt_df['image_name'])\n",
    "            ds_train,ds_val=(WSI_Pyramid(anno_df=df,\n",
    "                                         crop_pixel_size=crop_pixel_size,\n",
    "                                         pyramid_top_level=pyramid_top_level,\n",
    "                                         num_pyramid_levels=num_pyramid_levels,\n",
    "                                         num_pyramid_mask_levels=num_pyramid_mask_levels) for df in (df_train,df_val))\n",
    "            \n",
    "           \n",
    "            dl_train,dl_val=(dset.get_dl(batch_size=batch_size,kind=kind) for dset,kind in zip((ds_train,ds_val),('train','val')))\n",
    "            \n",
    "        \n",
    "            return dl_train,dl_val\n",
    "\n",
    "\n",
    "    def get_inference_dl(self,\n",
    "                         wsi_img_name,\n",
    "                        batch_size=32,\n",
    "                         \n",
    "                         **kwargs):\n",
    "        \n",
    "        wsi_path=self.inference_image_path/wsi_img_name\n",
    "        tissue_locs=self.preprocess(wsi_path)\n",
    "\n",
    "        ## pass extra args on num pyramid levels etc. to the dataset class\n",
    "        ds_inference=WSI_Inference(wsi_path= wsi_path,wsi_tissue_locs=tissue_locs,**kwargs)\n",
    "        dl_inference= ds_inference.get_dl( batch_size=batch_size)\n",
    "        return dl_inference\n",
    "    \n",
    "    \n",
    "    def get_inference_schema(self,sample_json='inference_schema.geojson'):\n",
    "        json_path=self.inference_path/sample_json\n",
    "        with open(json_path) as json_file:\n",
    "             schema = json.load(json_file)\n",
    "\n",
    "        return schema\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def run_inference(self,checkpoint_path:pathlib.Path,wsi_img_name:str,batch_size=32,max_iter=10,**kwargs):\n",
    "        inference_dl=self.get_inference_dl(wsi_img_name=wsi_img_name,batch_size=batch_size,anno_df=self.anno_df,**kwargs)\n",
    "       \n",
    "        ## setting pl_module to eval mode\n",
    "        inference_model=SegLightningModule.load_from_checkpoint(checkpoint_path)\n",
    "        inference_model.eval()\n",
    "\n",
    "        inference_filename='.'.join((wsi_img_name,'geojson'))\n",
    "        inference_filepath=self.inference_label_path/inference_filename\n",
    "        \n",
    "        inference_json=[]\n",
    "        with torch.no_grad():\n",
    "          for i,(top_left_b,img_b) in enumerate(tqdm(inference_dl)):\n",
    "               top_left_b,img_b=top_left_b.to(self.device),img_b.to(self.device)\n",
    "               if i>max_iter:\n",
    "                   break\n",
    "               pred_b= inference_model(img_b.to(torch.float32))\n",
    "               pred_b=torch.argmax(pred_b,dim=1)\n",
    "               pred_coords=torch.cat([torch.argwhere(pred)+top_left for pred,top_left in zip(pred_b,top_left_b) if pred.sum()>0])\n",
    "               coords_list=pred_coords.tolist()\n",
    "               if len(coords_list)>0:\n",
    "                  inference_schema=self.get_inference_schema()\n",
    "                  inference_schema['geometry'][ 'coordinates']=coords_list\n",
    "                  inference_json.append(inference_schema)\n",
    "                  ## write in append mode\n",
    "                  with open(inference_filepath, 'w') as json_file:\n",
    "                     json.dump(inference_json, json_file, indent=4)\n",
    "\n",
    "        \n",
    "       \n",
    "        \n",
    "      \n",
    "                   \n",
    "                   \n",
    "                   \n",
    "                \n",
    "        \n",
    "    \n",
    "    \n",
    "    def show_batch(self,kind='train',\n",
    "                   mask_color=torch.tensor((0,128,255)),\n",
    "                   num_pyramid_levels=4,\n",
    "                   crop_pixel_size=(512,512),\n",
    "                   pyramid_top_level={0: 1.0},\n",
    "                   show_batch_size=8,save=False,**kwargs,\n",
    "                   ):\n",
    "                   \n",
    "              \n",
    "\n",
    "          dl_train,dl_val=self.get_dls(batch_size=show_batch_size,\n",
    "                                                   num_pyramid_levels=num_pyramid_levels,\n",
    "                                                   num_pyramid_mask_levels=num_pyramid_levels,\n",
    "                                                   crop_pixel_size=crop_pixel_size,\n",
    "                                                   pyramid_top_level=pyramid_top_level\n",
    "                                                   )\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "          if kind=='train':\n",
    "            \n",
    "            img_b,mask_b=next(iter(dl_train))\n",
    "            #pdb.set_trace()\n",
    "\n",
    "          if kind=='val':\n",
    "            \n",
    "             img_b,mask_b=next(iter(dl_val))\n",
    "\n",
    "        \n",
    "          H,W=crop_pixel_size\n",
    "          img_b=img_b.reshape(batch_size*num_pyramid_levels,3,H,W)\n",
    "\n",
    "          #pdb.set_trace()\n",
    "          mask_b=mask_b.reshape(batch_size*num_pyramid_levels,1,H,W)\n",
    "\n",
    "        \n",
    "          fig = plt.figure(figsize=(32,24))\n",
    "          color_b=mask_b*mask_color.unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "          overlay_b=img_b+color_b\n",
    "          grid_img=make_grid(overlay_b,nrow=num_pyramid_levels)\n",
    "          \n",
    "          if save:\n",
    "            plt.imshow(grid_img.permute(1,2,0))\n",
    "              \n",
    "          plt.imshow(grid_img.permute(1,2,0))\n",
    "         \n",
    "\n",
    "         \n",
    "    \n",
    "       \n",
    "    \n",
    "    def run_training(self,name,epochs=10,\n",
    "                     num_pyramid_levels=4,\n",
    "                     num_pyramid_mask_levels=1,\n",
    "                     crop_pixel_size=(512,512),\n",
    "                     pyramid_top_level={0: 1.0},\n",
    "                     batch_size=32,\n",
    "                     **kwargs):\n",
    "\n",
    "        dl_train,dl_val=self.get_dls(batch_size=batch_size,\n",
    "                                     num_pyramid_levels=num_pyramid_levels,\n",
    "                                     num_pyramid_mask_levels=num_pyramid_mask_levels,\n",
    "                                     crop_pixel_size=crop_pixel_size,\n",
    "                                     pyramid_top_level=pyramid_top_level)\n",
    "\n",
    "        self.pl_module=SegLightningModule(in_channels=3*num_pyramid_levels,**kwargs)\n",
    "                                                   \n",
    "\n",
    "        \n",
    "        \n",
    "   \n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        self.trainer = pl.Trainer(max_epochs=epochs, accelerator=accelerator,callbacks=[EarlyStopping(monitor='Val_loss',\n",
    "                                                             patience=5,  verbose=True, mode='min' ),\n",
    "                                       TQDMProgressBar(refresh_rate=1)],\n",
    "                                logger=CSVLogger(flush_logs_every_n_steps=10,save_dir='runs',name=name))\n",
    "                                                                         \n",
    "                                                        \n",
    "                                                              \n",
    "                                               \n",
    "        \n",
    "        ## automatically saves model\n",
    "        self.trainer.fit(model=self.pl_module, train_dataloaders=dl_train,val_dataloaders=dl_val)\n",
    "            \n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbd0a68b-6bf9-4291-bdd2-d2967e8592c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner=experiment_runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b708efb-1201-4e52-b9ea-fc917ea33f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type             | Params\n",
      "--------------------------------------------------------\n",
      "0 | train_metrics      | MetricCollection | 0     \n",
      "1 | val_metrics        | MetricCollection | 0     \n",
      "2 | segmentation_model | UnetPlusPlus     | 26.1 M\n",
      "3 | loss_fn            | CrossEntropyLoss | 0     \n",
      "--------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.428   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 98/98 [02:14<00:00,  1.38s/it, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|██▎                                                       | 1/25 [00:00<00:05,  4.33it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|████▋                                                     | 2/25 [00:01<00:13,  1.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██████▉                                                   | 3/25 [00:02<00:18,  1.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|█████████▎                                                | 4/25 [00:03<00:19,  1.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███████████▌                                              | 5/25 [00:04<00:18,  1.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|█████████████▉                                            | 6/25 [00:05<00:18,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████████████████▏                                         | 7/25 [00:07<00:18,  1.02s/it]\u001b[A\n",
      "Validation DataLoader 0:  32%|██████████████████▌                                       | 8/25 [00:08<00:18,  1.11s/it]\u001b[A\n",
      "Validation DataLoader 0:  36%|████████████████████▉                                     | 9/25 [00:09<00:17,  1.10s/it]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████████████████████▊                                  | 10/25 [00:10<00:16,  1.09s/it]\u001b[A\n",
      "Validation DataLoader 0:  44%|█████████████████████████                                | 11/25 [00:11<00:15,  1.08s/it]\u001b[A\n",
      "Validation DataLoader 0:  48%|███████████████████████████▎                             | 12/25 [00:12<00:13,  1.07s/it]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████████████████████████▋                           | 13/25 [00:14<00:12,  1.08s/it]\u001b[A\n",
      "Validation DataLoader 0:  56%|███████████████████████████████▉                         | 14/25 [00:15<00:12,  1.09s/it]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████████████████████████████▏                      | 15/25 [00:16<00:10,  1.09s/it]\u001b[A\n",
      "Validation DataLoader 0:  64%|████████████████████████████████████▍                    | 16/25 [00:17<00:09,  1.08s/it]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████████████████████████████████▊                  | 17/25 [00:18<00:08,  1.08s/it]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████████████████████████████████                | 18/25 [00:19<00:07,  1.11s/it]\u001b[A\n",
      "Validation DataLoader 0:  76%|███████████████████████████████████████████▎             | 19/25 [00:21<00:06,  1.11s/it]\u001b[A\n",
      "Validation DataLoader 0:  80%|█████████████████████████████████████████████▌           | 20/25 [00:22<00:05,  1.13s/it]\u001b[A\n",
      "Validation DataLoader 0:  84%|███████████████████████████████████████████████▉         | 21/25 [00:23<00:04,  1.12s/it]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████████████████████████████████████████▏      | 22/25 [00:24<00:03,  1.12s/it]\u001b[A\n",
      "Validation DataLoader 0:  92%|████████████████████████████████████████████████████▍    | 23/25 [00:26<00:02,  1.15s/it]\u001b[A\n",
      "Validation DataLoader 0:  96%|██████████████████████████████████████████████████████▋  | 24/25 [00:27<00:01,  1.15s/it]\u001b[A\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 98/98 [02:44<00:00,  1.67s/it, v_num=0]\u001b[A\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 98/98 [02:44<00:00,  1.67s/it, v_num=0]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric Val_loss improved. New best score: 0.574\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 98/98 [02:47<00:00,  1.71s/it, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "runner.run_training(name='test_04',epochs=1,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7be7d409-8505-4969-8583-5d269550049e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                            | 10/2114 [07:53<27:40:11, 47.34s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_02\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mversion_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch=3-step=392.ckpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mwsi_img_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m4520_Phi172_S3T1R1.tiff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 172\u001b[0m, in \u001b[0;36mexperiment_runner.run_inference\u001b[1;34m(self, checkpoint_path, wsi_img_name, batch_size, max_iter, **kwargs)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m## write in append mode\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(inference_filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m--> 172\u001b[0m    \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43minference_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\json\\__init__.py:180\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m--> 180\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "runner.run_inference(checkpoint_path=Path('test_02')/'version_0'/'checkpoints'/'epoch=3-step=392.ckpt',wsi_img_name='4520_Phi172_S3T1R1.tiff',batch_size=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
