{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ff08aa-9221-4bba-b48a-73a05135429a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Polygon\n",
    "import pdb\n",
    "import  cv2\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd8c910b-ffae-46a0-a673-8b896ba0beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENSLIDE_PATH=Path('openslide-win64-20230414')\n",
    "\n",
    "OPENSLIDE_FOLDER=os.path.join(Path().resolve(),'openslide-win64-20230414','openslide-win64-20230414','bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5441f1db-0fa9-4e8b-9c38-4bebdd08d39d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if hasattr(os,'add_dll_directory'):\n",
    "    with os.add_dll_directory(OPENSLIDE_FOLDER):\n",
    "        import openslide\n",
    "else:\n",
    "    import openslide\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4c631c2-3dbd-47d9-af0b-96d24fc1fba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'openslide' from 'A:\\\\mini\\\\envs\\\\aipath\\\\Lib\\\\site-packages\\\\openslide\\\\__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db231725-36f9-4b6b-a114-8ee45c588184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataloading'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataloading\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mannotation_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnnotationParser\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dataloading'"
     ]
    }
   ],
   "source": [
    "\n",
    "from dataloading.src.annotation_parser import AnnotationParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e08874d6-0df7-47bc-bebc-ec278eeee67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloading.src.annotation_parser import AnnotationParser\n",
    "from dataloading.src.wsi_datasets_tst import WSI_Pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1820fada-acb5-4869-97ad-ecdcf288d43a",
   "metadata": {},
   "source": [
    "## Parse the json annotations to create a tabular data structure for all annotations and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3be9e30c-0201-49ae-981a-d05ef4664c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/annotation_parser.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile src/annotation_parser.py\n",
    "from __future__ import annotations\n",
    "from shapely.geometry import Polygon\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import json\n",
    "import pdb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class AnnotationParser():\n",
    "    \"\"\" Parses WSI Files and associated GeoJson annotations to create a composite data frame\n",
    "         from both.Also returns the annotations as line items for which th eparsing didn't work \n",
    "         Requires folder containing WSI files and Annotations\"\"\"\n",
    "    \n",
    "    def __init__(self,image_path:pathlib.Path,labels_path:pathlib.Path)->None:\n",
    "        self.image_path=image_path\n",
    "        self.labels_path=labels_path\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_img_df(self)->pd.DataFrame:\n",
    "        \"\"\"use openslide to get properties,shape,levels etc.for each image\"\"\"\n",
    "        img_paths=[self.image_path/fn for fn in os.listdir(self.image_path)  if (self.image_path/fn).suffix == '.tiff']\n",
    "        img_df=pd.DataFrame({'image_path':img_paths})\n",
    "        img_df=img_df.assign(image_name=[img_path.stem for img_path in img_paths],\n",
    "                             WSI_size=[openslide.OpenSlide(img_path).dimensions for img_path in img_paths],\n",
    "                      levels=[openslide.OpenSlide(img_path).level_count for img_path in img_paths],\n",
    "                    downsample_levels=[{level:downsample for level,downsample in enumerate(openslide.OpenSlide(img_path).level_downsamples)} \n",
    "                           for img_path in img_paths])\n",
    "        return img_df\n",
    "\n",
    "\n",
    "    \n",
    "    def get_coordinates_array(self,anno_row:pd.Series)->np.ndarray:\n",
    "        \"\"\"parse the annotation json (as a series of rows to get coordinates\n",
    "           of the annotation as an array of dim n_points X 2 \"\"\"\n",
    "        \n",
    "       \n",
    "        \n",
    "        try:\n",
    "            geom=anno_row['geometry']\n",
    "            coord_list=geom['coordinates']\n",
    "            geom_type=geom['type']\n",
    "\n",
    "             ### last 2 dimensions of every poly are n_points X 2\n",
    "            # for multipolygon the list of coordinates i nested 1 level deep\n",
    "\n",
    "             ## to get largest polygon if more than one get marked by mistake in  every annotation\n",
    "            \n",
    "            if geom_type=='Polygon':\n",
    "                largest_poly=max([np.array(poly).reshape((np.array(poly).shape)[-2:]) for poly in coord_list],key=len)\n",
    "\n",
    "            if geom_type=='MultiPolygon':\n",
    "                largest_poly= max([max([np.array(poly).reshape((np.array(poly).shape)[-2:]) for poly in coords],key=len) \n",
    "                                   for coords in coord_list],key=len)\n",
    "\n",
    "           \n",
    "            \n",
    "            return largest_poly\n",
    "\n",
    "        \n",
    "        except KeyError:\n",
    "            return 'coordinates_key_error'\n",
    "        \n",
    "       \n",
    "    \n",
    "    def get_class_name_and_color(self,anno_row:pd.Series)->Tuple[str,int]:\n",
    "        \"\"\"Parse annotation to return class name for each anno and color assigned in QuPath\"\"\"\n",
    "        \n",
    "     \n",
    "        \n",
    "        try:\n",
    "            classification=anno_row['properties']['classification']\n",
    "            class_name=classification['name']\n",
    "            class_color=classification['color']\n",
    "            return class_name,np.array(class_color)\n",
    "        \n",
    "        except KeyError:\n",
    "            \n",
    "            \n",
    "            return 'class_name_error',0\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "    def parse_json_file(self,json_path:pathlib.Path):\n",
    "        # need to be list of dicts (anno_list)\n",
    "        with open(json_path) as json_file:\n",
    "            anno_list = json.load(json_file)\n",
    "            if not isinstance(anno_list, list):\n",
    "                anno_list=[anno_list]\n",
    "            \n",
    "        \n",
    "        anno_df=pd.DataFrame(anno_list)\n",
    "        anno_df=anno_df.assign(image_name=json_path.stem)\n",
    "        #pdb.set_trace()\n",
    "        return anno_df\n",
    "\n",
    "        \n",
    "    \n",
    "    def get_anno_df(self,img_df:pd.DataFrame)->Tuple[pd.DataFrame,pd.DataFrame]:\n",
    "        \n",
    "        \"\"\" get annotation df including annotation for which parsing did not work (errored)\"\"\"\n",
    "        \n",
    "        jsons=[self.labels_path/fn for fn in os.listdir(self.labels_path)  if (self.labels_path/fn).suffix == '.geojson']\n",
    "        anno_df=pd.concat([self.parse_json_file(json_path) for json_path in jsons])\n",
    "        #pdb.set_trace()\n",
    "        #anno_df=pd.concat([pd.read_json(json,orient='records').assign(image_name=json.stem) for json in jsons],ignore_index=True)\n",
    "        \n",
    "        ## enrich with image specific attrs, total size,zoom levels etc.\n",
    "        anno_df=anno_df.merge(img_df,on='image_name')\n",
    "        ## add coordinates as np array and shapely polygon with transormed origin \n",
    "        anno_df['coordinates']=anno_df.apply(self.get_coordinates_array,1)\n",
    "        #pdb.set_trace()\n",
    "        anno_df['class_name'], anno_df['colour_RGB']=zip(*anno_df.apply(self.get_class_name_and_color,1))\n",
    "        \n",
    "        ## select the annos with errored coordinates (due to annotation issues)\n",
    "        errored=np.logical_or(anno_df['coordinates'].isin(['coordinates_key_error']),\n",
    "                              anno_df['class_name']=='class_name_error')\n",
    "        \n",
    "        errored_df=anno_df[errored]\n",
    "        anno_df=anno_df[~errored]\n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        ## use shapely to compute polygon attrs\n",
    "        anno_df['polygon']=anno_df.apply(lambda x:Polygon(x['coordinates']),1)\n",
    "        anno_df['area']=anno_df.apply(lambda x:x['polygon'].area,1)\n",
    "        anno_df['circumference']=anno_df.apply(lambda x:x['polygon'].length,1)\n",
    "        anno_df['bounds']=anno_df.apply(lambda x:np.array(x['polygon'].bounds).reshape((2,2)),1)\n",
    "       \n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        return anno_df,errored_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    def parse_annotations(self)->Tuple[pd.DataFrame,pd.DataFrame,pd.DataFrame]:\n",
    "        \"\"\" returns anno_df,img_df and errored df in that order \"\"\" \n",
    "        img_df=self.get_img_df()\n",
    "        anno_df,errored_df=self.get_anno_df(img_df)\n",
    "        \n",
    "        \n",
    "        return anno_df,img_df,errored_df\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8ff7fb-8e7b-4b7e-8299-35261710e1f4",
   "metadata": {},
   "source": [
    "## Define Pyramidal Dataset which samples concentric crops across multiple zoom levels with crop center coinciding with  the boundry of the annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d5f731d-4075-40e4-9c1f-f8db12b4fd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/wsi_datasets.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile src/wsi_datasets.py\n",
    "from __future__ import annotations\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from shapely.geometry import Polygon,MultiPolygon\n",
    "import  cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class WSI_Pyramid(Dataset):\n",
    "\n",
    "\n",
    "    \"\"\"Pytorch Dataset class representing a multiscale WSI dataset.inputs are img and anno dfs containing info \n",
    "         about WSI images and annotations.Pyramidal crops are sampled from the pyramid_top_levels (usually set to the most\n",
    "         zoomed in level in the tiff dataset although more than one level can be used) ,with a downsample factor of one . A crop is \n",
    "         chosen from the top level of size crop_sz X crop_sz and concentric crops of the same size are chosen in the next\n",
    "         num_pyramid_level levels.\"\"\"\n",
    "   \n",
    "    \n",
    "    def __init__(self,\n",
    "                 anno_df:pd.DataFrame,\n",
    "                 img_df:pd.DataFrame,\n",
    "                 crop_pixel_size:tuple=(512,512),\n",
    "                 transform=None,\n",
    "                 ## accomodating Qupaths 'Other' labels\n",
    "                 class2num={'Background':0,'Tumor':1,'Other':1},\n",
    "                 anno_crop_prob=0.1,\n",
    "                 \n",
    "                 ## default set to show all levels, mostly {0:1} top level is picked\n",
    "                 pyramid_top_level={0:1.0},\n",
    "                 num_pyramid_levels=4, \n",
    "                 num_pyramid_mask_levels=1,\n",
    "                 filter_flag=False,**kwargs)->None:\n",
    "       \n",
    "        \n",
    "       \n",
    "        self.anno_df=anno_df\n",
    "        self.img_df=img_df\n",
    "        ## the size in pixel of  each crop -size is kept same at \n",
    "        ## various zoom levels for batching\n",
    "        self.crop_pixel_size=crop_pixel_size\n",
    "        self.item_transform = transform\n",
    "        self.class2num= class2num\n",
    "        ## create on self.device\n",
    "        ## offsets to add to crop center to get vertices\n",
    "        self.offsets=np.array(self.crop_pixel_size)//2\n",
    "        ## get the downsample levels common in the entire dataset\n",
    "        self.common_downsample_levels=min(self.anno_df['downsample_levels'],key=len)\n",
    "        ## take the intersect of user provided ds levels and the ones present in the data\n",
    "        self.pyramid_top_level=pyramid_top_level\n",
    "        self.pyramid_all_levels=max(self.anno_df['downsample_levels'],key=len)\n",
    "        ## probability that random crop selected will be from within the annotation bound\n",
    "        ## as opposed to a crop chosen from a random image at a random location\n",
    "        self.anno_crop_prob=anno_crop_prob\n",
    "\n",
    "        \n",
    "        self.pyramid_top_idx=list(self.pyramid_top_level.keys())[0]\n",
    "        self.pyramid_top_downsample=list(self.pyramid_top_level.values())[0]\n",
    "        \n",
    "        self.num_pyramid_levels=num_pyramid_levels\n",
    "        self.num_pyramid_mask_levels=num_pyramid_mask_levels\n",
    "        assert self.num_pyramid_levels>= self.num_pyramid_mask_levels,'num_pyramid_levels used for inputs should be more than num of target maska'\n",
    "        \n",
    "        ## the actual levels of the tiff pyramid used as input to the model\n",
    "        self.pyramid_zoom_levels={idx:self.common_downsample_levels[idx] for idx in range(self.pyramid_top_idx,self.pyramid_top_idx+self.num_pyramid_levels)}\n",
    "        \n",
    "        self.filter_flag=filter_flag\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.anno_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def sample_crop_center(self,annotation_row:pd.Series):\n",
    "          \n",
    "        if np.random.rand()<self.anno_crop_prob:\n",
    "            ## case where crop center is sampled from the annotations bounds\n",
    "            \n",
    "            minxy, maxxy=annotation_row['bounds']\n",
    "            return np.random.randint(low=np.array(minxy)-self.offsets,high=np.array(maxxy)+self.offsets)\n",
    "\n",
    "        else:\n",
    "            ## case where random crop from all possible issue locs from the same image\n",
    "            tissue_locs=self.img_df.set_index('image_name',inplace=False).loc[annotation_row['image_name']]['tissue_location']\n",
    "            return  tissue_locs[np.random.randint(low=0,high=len(tissue_locs))]+self.offsets\n",
    "\n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "           \n",
    "        \n",
    "    \n",
    "    def get_pyramid_crops(self,annotation_row:pd.Series):\n",
    "        \n",
    "        \"\"\" get a random center crop at any possible zoom level from the periphery\n",
    "           of an annotation \"\"\"\n",
    "       \n",
    "\n",
    "       \n",
    "       \n",
    "        wsi_size,anno_coordinates=(np.array(x) for x in [annotation_row['WSI_size'],\n",
    "                                                             annotation_row['coordinates']])\n",
    "\n",
    "       \n",
    "        random_crop_center=torch.tensor(self.sample_crop_center(annotation_row))\n",
    "        # pdb.set_trace()\n",
    "        offsets_arr=torch.tensor([[-1,1],[1,1],[1,-1],[-1,-1]])*self.offsets   # 4 X 2\n",
    "        downsample_arr=torch.tensor(list(self.pyramid_zoom_levels.values())).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "       \n",
    "        pyramid_crops=offsets_arr.unsqueeze(0)*downsample_arr+random_crop_center.unsqueeze(0).unsqueeze(0)# Pyramid_Levels X 4 X 2  (one crop for each level)\n",
    "        pyramid_crops=np.array(pyramid_crops).astype(np.int32)\n",
    "\n",
    "        pyramid_top_lefts=pyramid_crops.min(axis=1)   # Pyramid_Levels X 2\n",
    "      \n",
    "       \n",
    "        ## get the top left of every sampled level in the pyr\n",
    "        return pyramid_crops,pyramid_top_lefts\n",
    "\n",
    "\n",
    "    def filter_crops_byWSIsize(self,wsi_size:tuple,all_crops:np.ndarray):\n",
    "        \n",
    "         \n",
    "    \n",
    "        max_bounds=np.max(all_crops,axis=1).values<wsi_size.unsqueeze(0)\n",
    "        min_bounds=np.min(all_crops,axis=1).values>torch.zeros_like(wsi_size.unsqueeze(0))\n",
    "        \n",
    "        ## get all feasible crops/tiles which are wholly within the WSI bounds, associated with that particular annotation\n",
    "        all_crops=all_crops[np.logical_and(max_bounds.all(axis=1),min_bounds.all(axis=1))]\n",
    "\n",
    "        ## if there is no possible crop that fits in the WSI image for a particular annotation and zoom level, retrn \n",
    "        ## empty tensors\n",
    "        \n",
    "         \n",
    "        return all_crops\n",
    "    \n",
    "   \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "    def get_mask_per_class(self,class_annotation_data:pd.DataFrame,crop:np.ndarray,\n",
    "                          downsample_factor:float)->torch.tensor:\n",
    "       \n",
    "        \"\"\"\"function to create masks of each class given the annotation data and crop(image) \n",
    "            coordinates=(4X2 shape) also the donsample factor of the crop to scale the polygon coords\"\"\"\n",
    "        annotation_class=class_annotation_data['class_name'].iloc[0]\n",
    "        annotation_num=self.class2num[annotation_class]\n",
    "        \n",
    "        \n",
    "        ## select the top left point of the crop\n",
    "        ## its the point with the min X and Y corrdinates (top left of image is origin)\n",
    "        \n",
    "        top_left=crop.min(axis=0)\n",
    "        \n",
    "        ## create a shapely polygon from crop to find intersections between annotations and crop\n",
    "        \n",
    "        crop_poly=Polygon(crop)\n",
    "        \n",
    "        ## create list of intersecting polygons with crop to fill with clss encoding\n",
    "        \n",
    "        intersects=[]\n",
    "        for poly in  class_annotation_data['polygon']:\n",
    "            if not crop_poly.intersects(poly):\n",
    "                continue\n",
    "            else:\n",
    "                intersect=crop_poly.intersection(poly)\n",
    "                \n",
    "                if isinstance(intersect,MultiPolygon):\n",
    "                    for inter in intersect.geoms:\n",
    "                        ext_coords=((np.array(inter.convex_hull.exterior.coords)-top_left)//downsample_factor).astype(np.int32)\n",
    "                        intersects.append(ext_coords)\n",
    "                elif isinstance(intersect,Polygon):\n",
    "                        ext_coords=((np.array(intersect.convex_hull.exterior.coords)-top_left)//downsample_factor).astype(np.int32)\n",
    "                        intersects.append(ext_coords)\n",
    "                else:\n",
    "                        continue\n",
    "                        \n",
    "                        \n",
    "                \n",
    "\n",
    "                    \n",
    "        mask=np.zeros(self.crop_pixel_size,dtype=np.uint8)\n",
    "        \n",
    "       \n",
    "        ## fill the intersected polygons within the mask\n",
    "        cv2.fillPoly(mask,intersects,color=annotation_num)\n",
    "        \n",
    "        return torch.tensor(mask,dtype=torch.uint8)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def read_slide_region(self,slide_obj:openslide.OpenSlide,top_left:np.ndarray,\n",
    "                         level:int):\n",
    "        \"\"\" returns the pixel RGB from WSI given a location,crop_size and level\"\"\"\n",
    "       \n",
    "        return slide_obj.read_region(tuple(top_left.astype(np.int32)),level,self.crop_pixel_size)\n",
    "\n",
    "    def get_dl(self,batch_size,kind,shuffle=True):\n",
    "        ## only shuffle the train dl not the validation one\n",
    "        shuffle=kind=='train'\n",
    "        return DataLoader(dataset=self,batch_size=batch_size,shuffle=shuffle)\n",
    "\n",
    "    def get_img_T(self,pyramid_top_lefts:np.ndarray,image_path:pathlib.Path):\n",
    "        \n",
    "\n",
    "        img_T=np.concatenate([np.array(self.read_slide_region(openslide.OpenSlide(image_path),sampled_top_left,zoom_level))[:,:,:-1] \n",
    "                  for zoom_level,sampled_top_left in zip(self.pyramid_zoom_levels,pyramid_top_lefts)],axis=2)\n",
    "    \n",
    "        img_T=torch.tensor(img_T).permute(2,0,1)\n",
    "        return img_T\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def get_msk_T(self,pyramid_crops:np.ndarray,image_anno_data:pd.DataFrame):\n",
    "        pyramid_msk=[]\n",
    "        for i in range(self.num_pyramid_mask_levels):\n",
    "            get_classwise_masks=partial(self.get_mask_per_class,\n",
    "                                              crop=pyramid_crops[i],\n",
    "                                              downsample_factor= list(self.pyramid_zoom_levels.values())[i])\n",
    "            \n",
    "            class_wise_masks=image_anno_data.groupby('class_name').apply(get_classwise_masks)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            ## stack the masks of various classes\n",
    "            stacked_masks=torch.stack(class_wise_masks.to_list(),dim=0)\n",
    "            \n",
    "            ## create a composite mask with higher class numbers taking precedence in case of ties\n",
    "            \n",
    "            composite_mask=stacked_masks.max(dim=0)\n",
    "            pyramid_msk.append(composite_mask.values)\n",
    "        \n",
    "        return torch.stack(pyramid_msk)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ## select annotation \n",
    "        anno_row=self.anno_df.iloc[index]\n",
    "        \n",
    "        ## select all annotations in the same image as indexed annotation\n",
    "        image_name,anno_class=anno_row['image_name'],anno_row['class_name']\n",
    "        dowsample_levels=anno_row['downsample_levels']\n",
    "        image_path=anno_row['image_path']\n",
    "        image_anno_data=self.anno_df[self.anno_df['image_name']==image_name]\n",
    "        \n",
    "        ## select pyramidal crops from N_levels zoom levels\n",
    "        pyramid_crops,pyramid_top_lefts=self.get_pyramid_crops(anno_row)\n",
    "\n",
    "        ## create a stack of pyramid crops centered at the annotation with as many zoom levels as descibed by pyramid_levels\n",
    "        img_T=self.get_img_T(pyramid_top_lefts,image_path)\n",
    "        mask_T=self.get_msk_T(pyramid_crops,image_anno_data)\n",
    "    \n",
    "        return img_T,mask_T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class WSI_Inference(WSI_Pyramid):\n",
    "\n",
    "\n",
    "    \"\"\"Pytorch Dataset class to perform inference on a WSI.Input is the tissue locations on a WSI which are obtained after\n",
    "      removal of background. Inference is run on crops of 128X128 extracted from these locations.Inherits from the pyramid\n",
    "      parent class to make available common convenience functions\"\"\"\n",
    "\n",
    "    \n",
    "   \n",
    "    \n",
    "    def __init__(self,\n",
    "                 wsi_path:pathlib.Path,\n",
    "                 wsi_tissue_locs:np.ndarray,\n",
    "                 **kwargs)->None:\n",
    "        ## init the pyramidal dataset\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.wsi_path= wsi_path\n",
    "        self.wsi_tissue_locs=wsi_tissue_locs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len( self.wsi_tissue_locs)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        top_left=torch.tensor(self.wsi_tissue_locs[index])\n",
    "        crop_center=top_left+self.offsets\n",
    "        downsample_arr=torch.tensor(list(self.pyramid_zoom_levels.values()))\n",
    "        pyramid_top_lefts=crop_center-self.offsets.unsqueeze(0)* downsample_arr.unsqueeze(1)\n",
    "        pyramid_top_lefts=np.array( pyramid_top_lefts).astype(np.int32)\n",
    "        img_T=self.get_img_T(pyramid_top_lefts,self.wsi_path)\n",
    "        ## return locations and pyramidal images for inference\n",
    "        return top_left,img_T\n",
    "\n",
    "    def get_dl(self,batch_size):\n",
    "        return DataLoader(dataset=self,batch_size=batch_size,shuffle=False)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "       \n",
    "        \n",
    "       \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca21411-ccd6-4b56-ad84-16d684ea2ed1",
   "metadata": {},
   "source": [
    "## Define PL module for segmentation, can have various architecture and encoder types as provided in CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e0d2e69-a276-4a1a-be1f-89b90c6735f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/segmodule.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile src/segmodule.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import Accuracy,Precision, Recall,JaccardIndex,Dice\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Define your PyTorch Lightning module (inherits from pl.LightningModule)\n",
    "class SegLightningModule(pl.LightningModule):\n",
    "    def __init__(self,in_channels=3,num_classes=2, arch_name='UnetPlusPlus'\n",
    "                 ,encoder_name='resnet34',crossentropy_weights=(3.0,8.0),lr=1e-3,**kwargs):\n",
    "        super(SegLightningModule, self).__init__()\n",
    "        # Define your model architecture here\n",
    "        \n",
    "        self.train_metrics = MetricCollection(prefix='Train',metrics=[\n",
    "                            Accuracy(task='binary' ),\n",
    "                            Precision(task='binary'),\n",
    "                            Recall(task='binary'),\n",
    "                           JaccardIndex(task='binary')])\n",
    "        self.val_metrics = MetricCollection(prefix='Val',metrics=[\n",
    "                            Accuracy(task='binary' ),\n",
    "                            Precision(task='binary'),\n",
    "                            Recall(task='binary'),\n",
    "                           JaccardIndex(task='binary')])\n",
    "        \n",
    "        arch=getattr(smp, arch_name)\n",
    "        self.segmentation_model=arch(encoder_name=encoder_name,in_channels=in_channels,classes=num_classes)\n",
    "        self.loss_fn=nn.CrossEntropyLoss(weight=torch.tensor(crossentropy_weights))\n",
    "        self.lr=lr\n",
    "        #self.register_buffer('mask_color',torch.tensor([255,255,255]).to(torch.uint8).unsqueeze(0).unsqueeze(2).unsqueeze(3))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass of your model\n",
    "        \n",
    "        return self.segmentation_model(x)\n",
    "    \n",
    "    def flattened_cross_entropy_loss(self,inp,tgt):\n",
    "        \n",
    "        tgt=tgt.flatten(start_dim=-2)\n",
    "        inp=inp.flatten(start_dim=-2)\n",
    "      \n",
    "        return self.loss_fn(inp, tgt)\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Define the training step\n",
    "      \n",
    "        img_b,mask_b=batch\n",
    "        img_b=img_b.to(torch.float32)\n",
    "        #taking the mask at the highest zoom  as target\n",
    "        mask_b=mask_b[:,0,:,:].to(torch.int64) \n",
    "        \n",
    "        y_pred_logits = self(img_b)\n",
    "        loss = self.flattened_cross_entropy_loss(y_pred_logits, mask_b)\n",
    "        y_pred=y_pred_logits.argmax(axis=1)\n",
    "        \n",
    "        metrics=self.train_metrics(y_pred,mask_b)\n",
    "        metrics.update({'Train_loss':loss,'Train_pct_foreground':mask_b.float().mean(),'Train_IOU':(y_pred*mask_b).sum()/(y_pred.sum()+mask_b.sum())})\n",
    "\n",
    "        ## save pred and ground truth masks every epoch\n",
    "        ## only log 8 masks from batch\n",
    "        \n",
    "        self.log_dict(metrics,on_step=False,on_epoch=True,prog_bar=False)  # Log the training loss for TensorBoard\n",
    "\n",
    "        # if batch_idx==0:\n",
    "        #     grid_pred=make_grid(y_pred[:8].to(torch.int8).unsqueeze(1)*self.mask_color,nrow=1).permute(1,2,0)\n",
    "        #     grid_targ=make_grid(mask_b[:8].to(torch.int8).unsqueeze(1)*self.mask_color,nrow=1).permute(1,2,0)\n",
    "        #     grid=torch.cat([grid_pred,grid_targ],dim=1)\n",
    "        #     plt.imsave(Path(self.logger.log_dir)/'.'.join(['Train_epoch'+str(self.current_epoch),'jpg']),grid)\n",
    "            \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Define the training step\n",
    "      \n",
    "        img_b,mask_b=batch\n",
    "        img_b=img_b.to(torch.float32)\n",
    "        #taking the mask at the highest zoom  as target\n",
    "        mask_b=mask_b[:,0,:,:].to(torch.int64) \n",
    "\n",
    "       \n",
    "        y_pred_logits = self(img_b)\n",
    "        loss = self.flattened_cross_entropy_loss(y_pred_logits, mask_b)\n",
    "        y_pred=y_pred_logits.argmax(axis=1)\n",
    "        \n",
    "        metrics=self.val_metrics(y_pred,mask_b)\n",
    "        metrics.update({'Val_loss':loss,'Val_pct_foreground':mask_b.float().mean() ,'Val_IOU':(y_pred*mask_b).sum()/(y_pred.sum()+mask_b.sum())})\n",
    "        self.log_dict(metrics,on_step=False,on_epoch=True,prog_bar=False)  # Log the training loss for TensorBoard\n",
    "        \n",
    "        # if batch_idx==0:\n",
    "        #     grid_pred=make_grid(y_pred[:8].to('torch.uint8')unsqueeze(1)*self.mask_color,nrow=1).permute(1,2,0)\n",
    "        #     grid_targ=make_grid(mask_b[:8].to('torch.uint8').unsqueeze(1)*self.mask_color,nrow=1).permute(1,2,0)\n",
    "        #     grid=torch.cat([grid_pred,grid_targ],dim=1)\n",
    "        #     plt.imsave(Path(self.logger.log_dir)/'.'.join(['Train_epoch'+str(self.current_epoch),'jpg']),grid)\n",
    "            \n",
    "        \n",
    "        return loss\n",
    "        \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define your optimizer\n",
    "        optimizer = optim.Adam(self.parameters(),lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "class InferenceLightningModule(pl.LightningModule):\n",
    "    def __init__(self,seg_module:SegLightningModule,\n",
    "                 WSI_shape,downsample_factor=16,**kwargs):\n",
    "       super(InferenceLightningModule, self).__init__()\n",
    "        # Define your model architecture here\n",
    "        \n",
    "       self.seg_module=seg_module\n",
    "     \n",
    "       W,H=WSI_shape\n",
    "       self.downsample_factor=downsample_factor\n",
    "       ## np array like indexing is reversed of slide dims\n",
    "       self.WSI_mask=np.zeros((H//downsample_factor,W//downsample_factor))\n",
    "       \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass of your model\n",
    "        \n",
    "        return  self.seg_module(x)\n",
    "    \n",
    "    \n",
    "\n",
    "    def get_inference_schema(self,sample_json='inference_schema.geojson'):\n",
    "        json_path=self.inference_path/sample_json\n",
    "        with open(json_path) as json_file:\n",
    "             schema = json.load(json_file)\n",
    "\n",
    "        return schema\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Define the training step\n",
    "      \n",
    "        top_left_b, img_b=batch\n",
    "         \n",
    "        _,_,h,w=img_b.shape\n",
    "        h_ds,w_ds=tuple(np.array((h,w))//self.downsample_factor)\n",
    "        \n",
    "        img_b=img_b.to(torch.float32)\n",
    "        y_pred_logits = self(img_b)\n",
    "        y_pred=y_pred_logits.argmax(axis=1)\n",
    "        y_pred_ds=transforms.Resize((h//self.downsample_factor,w//self.downsample_factor))(y_pred)\n",
    "        ##  append masks for each batch\n",
    "        for top_left,pred in zip(top_left_b.cpu(),y_pred_ds.cpu()):\n",
    "            Y_start,X_start=tuple(np.array(top_left)//self.downsample_factor)\n",
    "            try:\n",
    "                self.WSI_mask[Y_start:Y_start+h_ds,X_start:X_start+w_ds]=pred\n",
    "            except:\n",
    "                pdb.set_trace()\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "     \n",
    "        \n",
    "       \n",
    "      \n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1ea88-a30d-4d2d-ad64-b367886f78b3",
   "metadata": {},
   "source": [
    "## Define Runner class to handle training and inference and preprocessing on whole slide images. Preprocessing step uses the thumbnail image to remove useless (grey) image areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "796af1a7-3550-43e5-9a43-7019a0b52daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/runner.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile src/runner.py\n",
    "from __future__ import annotations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar,EarlyStopping\n",
    "\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class experiment_runner():\n",
    "    \n",
    "    \n",
    "    def __init__(self,root=Path('training_data')\n",
    "                 \n",
    "                 \n",
    "               ):\n",
    "            \n",
    "            self.label_path=root/'labels'\n",
    "            self.image_path=root/'images'\n",
    "            \n",
    "            \n",
    "            self.inference_path=Path('inference')\n",
    "\n",
    "            self.inference_image_path=self.inference_path/'images'\n",
    "            ## path to store predictions (as json files)\n",
    "            self.inference_label_path=self.inference_path/'labels'\n",
    "            \n",
    "            self.parser=AnnotationParser(self.image_path,self.label_path)\n",
    "            self.anno_df,self.img_df,self.errored=self.parser.parse_annotations()\n",
    "            \n",
    "            ## device to perform inference on\n",
    "            self.device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "            \n",
    "            \n",
    "          \n",
    "            \n",
    "    def preprocess(self,img_path:pathlib.Path,downsample_factor=512,gray_background=np.array([236, 236, 236]),tol=0)->np.ndarray:\n",
    "        \"\"\" preprocess WSI to remove grey areas returns pixel locations\n",
    "            from the downsampled (thumbnail) image where the tissue exists\n",
    "              \"\"\"\n",
    "        \n",
    "        \n",
    "        slide=openslide.OpenSlide(img_path)\n",
    "        ## slide dimensions and read region are in x,y format and not y,x like python array indexing\n",
    "        W,H=slide.dimensions\n",
    "       \n",
    "        thumbnail_img=np.array(slide.get_thumbnail((W//downsample_factor,H//downsample_factor)))\n",
    "        w,h,c=thumbnail_img.shape\n",
    "        \n",
    "        rgb_upper_bound=gray_background+tol\n",
    "        rgb_lower_bound=gray_background-tol\n",
    "        grey_mask = cv2.inRange( thumbnail_img, rgb_lower_bound,  rgb_upper_bound)\n",
    "        \n",
    "        \n",
    "        num_comps, labelled_mask = cv2.connectedComponents(~grey_mask)\n",
    "\n",
    "     \n",
    "        tissue_mask=[]\n",
    "        \n",
    "        for i in range(1,num_comps):\n",
    "            comp_mask=(labelled_mask==i)\n",
    "            #pdb.set_trace()\n",
    "            unique_rgb_vals=np.unique(thumbnail_img.reshape((h*w,c))[comp_mask.flatten()],axis=0)\n",
    "            if len(unique_rgb_vals)>1:\n",
    "              tissue_mask.append(comp_mask)\n",
    "\n",
    "        tissue_mask=np.stack(tissue_mask,axis=0).max(axis=0)\n",
    "        ## coordinates need to be flipped because openslide expects corrdinates in x,y as opposed to y,x indexing of numpy\n",
    "        coords=np.flip(np.argwhere(tissue_mask) ,axis=1)          \n",
    "              \n",
    "          \n",
    "        return coords*downsample_factor,( W,H)\n",
    "        \n",
    "        \n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    ## configure training params here\n",
    "    def get_dls(self,\n",
    "               batch_size=32,\n",
    "                num_pyramid_levels=4,\n",
    "                num_pyramid_mask_levels=1,\n",
    "                crop_pixel_size=(512,512),\n",
    "                pyramid_top_level={0: 1.0},\n",
    "                ** kwargs\n",
    "                \n",
    "                ):\n",
    "\n",
    "            d_factor,d_factor= crop_pixel_size\n",
    "            ##compute tissue locations for training images\n",
    "            t_locs=self.img_df['image_path'].apply(lambda path:self.preprocess(img_path=path,downsample_factor=d_factor)[0],1)\n",
    "            self.img_df=self.img_df.assign(tissue_location=t_locs)\n",
    "            self.img_df['inference_len']=self.img_df['tissue_location'].apply(len,1)\n",
    "\n",
    "            #filtering out image_name with less than 2 annotations for stratifying using train_test_split\n",
    "            vc=self.anno_df['image_name'].value_counts()\n",
    "            filt_df=self.anno_df.merge(vc.reset_index())\n",
    "            ## select images with more than the median number of annotations\n",
    "            filt_df=filt_df[filt_df['count']>50]\n",
    "            #filt_df=filt_df[filt_df['count']==max(vc)]\n",
    "          \n",
    "            df_train,df_val=train_test_split(filt_df,test_size=0.2,random_state=42,stratify=filt_df['image_name'])\n",
    "            ds_train,ds_val=(WSI_Pyramid(anno_df=df,\n",
    "                                         img_df=self.img_df,\n",
    "                                         crop_pixel_size=crop_pixel_size,\n",
    "                                         pyramid_top_level=pyramid_top_level,\n",
    "                                         num_pyramid_levels=num_pyramid_levels,\n",
    "                                         num_pyramid_mask_levels=num_pyramid_mask_levels,**kwargs) for df in (df_train,df_val))\n",
    "            \n",
    "           \n",
    "            dl_train,dl_val=(dset.get_dl(batch_size=batch_size,kind=kind) for dset,kind in zip((ds_train,ds_val),('train','val')))\n",
    "            \n",
    "        \n",
    "            return dl_train,dl_val\n",
    "\n",
    "\n",
    "    def get_inference_dl(self,\n",
    "                        wsi_path,\n",
    "                        tissue_locs,\n",
    "                        batch_size=32,\n",
    "                         \n",
    "                       \n",
    "                         \n",
    "                         **kwargs):\n",
    "        \n",
    "       \n",
    "\n",
    "        ## pass extra args on num pyramid levels etc. to the dataset class\n",
    "        ds_inference=WSI_Inference(wsi_path= wsi_path,wsi_tissue_locs=tissue_locs,**kwargs)\n",
    "        dl_inference= ds_inference.get_dl( batch_size=batch_size)\n",
    "        return dl_inference\n",
    "    \n",
    "    \n",
    "    def get_inference_schema(self,sample_json='inference_schema.geojson'):\n",
    "        json_path=self.inference_path/sample_json\n",
    "        with open(json_path) as json_file:\n",
    "             schema = json.load(json_file)\n",
    "\n",
    "        return schema\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def run_inference(self,checkpoint_path:pathlib.Path,wsi_img_name:str,batch_size=32,downsample_factor=16,\n",
    "                      \n",
    "                      **kwargs):\n",
    "         \n",
    "        wsi_path=self.inference_image_path/wsi_img_name\n",
    "        tissue_locs,WSI_shape=self.preprocess(wsi_path)\n",
    "        \n",
    "        \n",
    "        ## get dl,pass dl specefic params here\n",
    "        inference_dl=self.get_inference_dl(wsi_path=wsi_path,tissue_locs=tissue_locs,batch_size=batch_size,anno_df=self.anno_df,**kwargs)\n",
    "       \n",
    "        ## get model (previously trained from checkpoint to perform inference\n",
    "        Seg_model=SegLightningModule.load_from_checkpoint(checkpoint_path)\n",
    "        inference_model= InferenceLightningModule(seg_module=Seg_model,WSI_shape=WSI_shape,downsample_factor=downsample_factor)\n",
    "\n",
    "        inference_trainer=pl.Trainer(logger=None,callbacks=[],accelerator='gpu',max_epochs=1)\n",
    "        inference_trainer.test(model=inference_model,dataloaders=inference_dl)\n",
    "\n",
    "        downsampled_mask=inference_model.WSI_mask\n",
    "\n",
    "        return  downsampled_mask\n",
    "       \n",
    "\n",
    "\n",
    "    def show_batch(self,kind='train',\n",
    "                   mask_color=torch.tensor((0,128,255)),\n",
    "                   num_pyramid_levels=4,\n",
    "                   crop_pixel_size=(512,512),\n",
    "                   pyramid_top_level={0: 1.0},\n",
    "                   show_batch_size=8,save=False,**kwargs,\n",
    "                   ):\n",
    "                   \n",
    "              \n",
    "\n",
    "          dl_train,dl_val=self.get_dls(batch_size=show_batch_size,\n",
    "                                                   num_pyramid_levels=num_pyramid_levels,\n",
    "                                                   num_pyramid_mask_levels=num_pyramid_levels,\n",
    "                                                   crop_pixel_size=crop_pixel_size,\n",
    "                                                   pyramid_top_level=pyramid_top_level,\n",
    "                                                   ** kwargs\n",
    "                                                   )\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "          if kind=='train':\n",
    "            \n",
    "            img_b,mask_b=next(iter(dl_train))\n",
    "            #pdb.set_trace()\n",
    "\n",
    "          if kind=='val':\n",
    "            \n",
    "             img_b,mask_b=next(iter(dl_val))\n",
    "\n",
    "        \n",
    "          H,W=crop_pixel_size\n",
    "          img_b=img_b.reshape(show_batch_size*num_pyramid_levels,3,H,W)\n",
    "\n",
    "          #pdb.set_trace()\n",
    "          mask_b=mask_b.reshape(show_batch_size*num_pyramid_levels,1,H,W)\n",
    "\n",
    "        \n",
    "          fig = plt.figure(figsize=(32,24))\n",
    "          color_b=mask_b*mask_color.unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "          overlay_b=img_b+color_b\n",
    "          grid_img=make_grid(overlay_b,nrow=num_pyramid_levels)\n",
    "          \n",
    "          if save:\n",
    "            return grid_img.permute(1,2,0)\n",
    "              \n",
    "          plt.imshow(grid_img.permute(1,2,0))\n",
    "         \n",
    "\n",
    "         \n",
    "    \n",
    "       \n",
    "    \n",
    "    def run_training(self,name,epochs=10,\n",
    "                     num_pyramid_levels=4,\n",
    "                     num_pyramid_mask_levels=1,\n",
    "                     crop_pixel_size=(512,512),\n",
    "                     pyramid_top_level={0: 1.0},\n",
    "                     batch_size=32,\n",
    "                     patience=10,\n",
    "                     **kwargs):\n",
    "\n",
    "        dl_train,dl_val=self.get_dls(batch_size=batch_size,\n",
    "                                     num_pyramid_levels=num_pyramid_levels,\n",
    "                                     num_pyramid_mask_levels=num_pyramid_mask_levels,\n",
    "                                     crop_pixel_size=crop_pixel_size,\n",
    "                                     pyramid_top_level=pyramid_top_level,\n",
    "                                      **kwargs)\n",
    "\n",
    "        self.pl_module=SegLightningModule(in_channels=3*num_pyramid_levels,**kwargs)\n",
    "                                                   \n",
    "\n",
    "        \n",
    "        \n",
    "   \n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        self.trainer = pl.Trainer(max_epochs=epochs, accelerator=accelerator,callbacks=[EarlyStopping(monitor='Train_loss',\n",
    "                                                             verbose=True, mode='min',patience=patience ),\n",
    "                                       TQDMProgressBar(refresh_rate=1)],\n",
    "                                       logger=CSVLogger(flush_logs_every_n_steps=10,save_dir='runs',name=name))\n",
    "                                                                         \n",
    "                                                        \n",
    "                                                              \n",
    "                                               \n",
    "        \n",
    "        ## automatically saves model\n",
    "        self.trainer.fit(model=self.pl_module, train_dataloaders=dl_train,val_dataloaders=dl_val)\n",
    "            \n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68235cfe-d3cd-4c7e-a7f7-23e43ee692bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runner=experiment_runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18d19713-66e4-4eb7-8a8d-d5d363fc0326",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type             | Params\n",
      "--------------------------------------------------------\n",
      "0 | train_metrics      | MetricCollection | 0     \n",
      "1 | val_metrics        | MetricCollection | 0     \n",
      "2 | segmentation_model | UnetPlusPlus     | 26.1 M\n",
      "3 | loss_fn            | CrossEntropyLoss | 0     \n",
      "--------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.315   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                 | 0/564 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 6.00 GiB total capacity; 5.17 GiB already allocated; 0 bytes free; 5.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreduced_lr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_pyramid_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcrop_pixel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 266\u001b[0m, in \u001b[0;36mexperiment_runner.run_training\u001b[1;34m(self, name, epochs, num_pyramid_levels, num_pyramid_mask_levels, crop_pixel_size, pyramid_top_level, batch_size, patience, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39mepochs, accelerator\u001b[38;5;241m=\u001b[39maccelerator,callbacks\u001b[38;5;241m=\u001b[39m[EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    257\u001b[0m                                                      verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m,patience\u001b[38;5;241m=\u001b[39mpatience ),\n\u001b[0;32m    258\u001b[0m                                TQDMProgressBar(refresh_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)],\n\u001b[0;32m    259\u001b[0m                                logger\u001b[38;5;241m=\u001b[39mCSVLogger(flush_logs_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns\u001b[39m\u001b[38;5;124m'\u001b[39m,name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m## automatically saves model\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpl_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:529\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    527\u001b[0m model \u001b[38;5;241m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 529\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:42\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     45\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:568\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[0;32m    559\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[0;32m    560\u001b[0m )\n\u001b[0;32m    562\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    564\u001b[0m     ckpt_path,\n\u001b[0;32m    565\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    566\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    567\u001b[0m )\n\u001b[1;32m--> 568\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:973\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 973\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    978\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1016\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1016\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:201\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:354\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(combined_loader)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:133\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:218\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:185\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m         closure()\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:260\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:144\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    141\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    147\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1256\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1220\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1223\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1224\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer`\u001b[39;00m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m    calls the optimizer.\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;124;03m                    pg[\"lr\"] = lr_scale * self.learning_rate\u001b[39;00m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1256\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:155\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:225\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[1;32m--> 225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:114\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\torch\\optim\\adam.py:121\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 121\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    124\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:101\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     91\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     92\u001b[0m     optimizer: Optimizer,\n\u001b[0;32m     93\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m     94\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 126\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:307\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m    306\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[1;32m--> 307\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:291\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 291\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    294\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:367\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtrain_step_context():\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, TrainingStep)\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 59\u001b[0m, in \u001b[0;36mSegLightningModule.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m#taking the mask at the highest zoom  as target\u001b[39;00m\n\u001b[0;32m     57\u001b[0m mask_b\u001b[38;5;241m=\u001b[39mmask_b[:,\u001b[38;5;241m0\u001b[39m,:,:]\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint64) \n\u001b[1;32m---> 59\u001b[0m y_pred_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflattened_cross_entropy_loss(y_pred_logits, mask_b)\n\u001b[0;32m     61\u001b[0m y_pred\u001b[38;5;241m=\u001b[39my_pred_logits\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[8], line 41\u001b[0m, in \u001b[0;36mSegLightningModule.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Define the forward pass of your model\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msegmentation_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\segmentation_models_pytorch\\base\\model.py:30\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input_shape(x)\n\u001b[0;32m     29\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m---> 30\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_head(decoder_output)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_head \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\segmentation_models_pytorch\\decoders\\unetplusplus\\decoder.py:135\u001b[0m, in \u001b[0;36mUnetPlusPlusDecoder.forward\u001b[1;34m(self, *features)\u001b[0m\n\u001b[0;32m    133\u001b[0m             cat_features \u001b[38;5;241m=\u001b[39m [dense_x[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense_l_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, dense_l_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m    134\u001b[0m             cat_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(cat_features \u001b[38;5;241m+\u001b[39m [features[dense_l_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 135\u001b[0m             dense_x[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdepth_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense_l_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdepth_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdense_l_i\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdense_x\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdepth_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdense_l_i\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m dense_x[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m](dense_x[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dense_x[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\segmentation_models_pytorch\\decoders\\unetplusplus\\decoder.py:41\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[1;34m(self, x, skip)\u001b[0m\n\u001b[0;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention1(x)\n\u001b[0;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[1;32m---> 41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention2(x)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mA:\\mini\\envs\\aipath\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 6.00 GiB total capacity; 5.17 GiB already allocated; 0 bytes free; 5.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "runner.run_training(name='reduced_lr',epochs=40,patience=10,lr=1e-4,num_pyramid_levels=1,crop_pixel_size=(1024,1024),batch_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
